{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Detection from chest X-Ray images using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pablo Lázaro Herrasti and Rubén Barco Terrones\n",
    "\n",
    "In this Notebook we have collected all the steps and code that we have used during the developing of this research. All the sections of these Notebook are going to be explained below. If you have any doubt, question or suggestion, ask Pablo or Ruben via e-mail. You have our information on the [GitHub](https://github.com/polazaro/Covid-19-Detection) of the publication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "First of all, we are going to import all the libraries and functions that we are going to use in the project. We are going to use **Keras** over **Tensorflow** and some **Scikit-Learn** functions to compute the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T18:31:26.570305Z",
     "start_time": "2020-04-12T18:31:24.439481Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential, Model \n",
    "from tensorflow.keras.layers import concatenate, Input, Dropout, Flatten, Dense, GlobalAveragePooling2D,Conv2D,MaxPooling2D,BatchNormalization\n",
    "from tensorflow.keras import backend as k \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories\n",
    "\n",
    "Here, we use one cell to define all the paths that we are going to use during the Notebook. It is needed to say that we are going to have a main directory called `dir_covid`. This directory contains all the following subdirectories:\n",
    "* **`COVID-19_TRAIN`**: Contains all the chest X-Ray images which tested possitive in COVID-19 that we are going to use in the **training** process. \n",
    "* **`COVID-19_TRAIN_AUG`**: The same as before but with data augmentation\n",
    "* **`COVID-19_TEST`**: Contains all the chest X-Ray images which tested possitive in COVID-19 that we are going to use for **test**.\n",
    "* **`COVID-19_VAL`**: Contains all the chest X-Ray images which tested possitive in COVID-19 that we are going to use as **validation** set.\n",
    "* **`NORMAL`**: All the **healthy** images (train, validation and test).\n",
    "* **`Viral Pneumonia`**: All the **Viral Pneumonia** images (train, validation and test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T18:31:26.601180Z",
     "start_time": "2020-04-12T18:31:26.597189Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_covid = '.../Covid-19-Detection/'\n",
    "\n",
    "# Here we have to select only one of the following two directories (without and with DataAugmentation)\n",
    "dir_covid_images_train  = dir_covid + 'COVID-19_TRAIN/'      # With DataAug\n",
    "dir_covid_images_train  = dir_covid + 'COVID-19_TRAIN_AUG/'  # Without DataAug\n",
    "\n",
    "dir_covid_images_test   = dir_covid + 'COVID-19_TEST/'\n",
    "dir_covid_images_val    = dir_covid + 'COVID-19_VAL/'\n",
    "dir_normal_images       = dir_covid + 'NORMAL/'\n",
    "dir_pneumonia_images    = dir_covid + 'Viral Pneumonia/'\n",
    "\n",
    "# Here we join both normal and viral pneumonia dirs in a list to use them in a loop easily\n",
    "all_dir_images = [dir_normal_images, dir_pneumonia_images]\n",
    "\n",
    "# Here we define the metadata files for the three classes\n",
    "dir_covid_metadata      = dir_covid + 'COVID-19.metadata.xlsx'\n",
    "dir_normal_metadata     = dir_covid + 'NORMAL.metadata.xlsx'\n",
    "dir_pneumonia_metadata  = dir_covid + 'Viral Pneumonia.matadata.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Pre-processing Data\n",
    "\n",
    "In this part we start loading the metadata files. In this case we only need to load the healthy and viral pneumonia metadata. We are going to assign one label to each class:\n",
    "* **COVID-19** is **0**\n",
    "* **Healthy** (normal) is **1**\n",
    "* **Viral Pneumonia** (pneumonia) is **2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T18:31:26.754834Z",
     "start_time": "2020-04-12T18:31:26.624174Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata_normal = pd.read_excel(dir_normal_metadata)\n",
    "metadata_normal['label'] = 1\n",
    "metadata_pneumonia = pd.read_excel(dir_pneumonia_metadata)\n",
    "metadata_pneumonia['label'] = 2\n",
    "metadata_all = {dir_normal_images:metadata_normal, dir_pneumonia_images:metadata_pneumonia}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation\n",
    "\n",
    "The Data Augmentation is going to be performed with the Augmentator library for Python. This library has a lot of possible ways to increase the number of samples of our database, like fliping images, rotate them, apply distorsions, etc. To learn more about them you can visit this [link](https://github.com/mdbloice/Augmentor).\n",
    "\n",
    "**NOTE**: If you have already done your DataAugmentation, you can skip this two cells. Just change the previous directory to point to your data augmentated train folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T18:31:26.829603Z",
     "start_time": "2020-04-12T18:31:26.826605Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Augmentor\n",
    "\n",
    "# This command creates a folder with name 'output' inside the dir_covid_images_train directory in which all the new\n",
    "# images are going to be created. After the data augmenation, we have to merge the images from 'output' with the original \n",
    "# COVID-19 data to obtain all the COVID-19 images\n",
    "p = Augmentor.Pipeline(dir_covid_images_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T18:31:26.901375Z",
     "start_time": "2020-04-12T18:31:26.898385Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p.rotate(probability=0.7, max_left_rotation=15, max_right_rotation=15)\n",
    "p.flip_left_right(probability=0.7)\n",
    "p.sample(len(os.listdir(dir_covid_images_train))*5)\n",
    "p.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Image Data\n",
    "\n",
    "In the following three cells we are only going to load the images from healthy and pneumonia cases because we can split them in train, validation and test using the `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T18:32:00.204275Z",
     "start_time": "2020-04-12T18:31:26.977174Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reading Image data and converting it into pixels and separating class labels\n",
    "Data=[]\n",
    "Label=[]\n",
    "\n",
    "for dir_images in all_dir_images:\n",
    "    files = os.listdir(dir_images)\n",
    "    for index, row in metadata_all[dir_images].iterrows():\n",
    "        Label.append(row['label'])\n",
    "        filename=os.path.join(dir_images, files[index])\n",
    "        im=image.load_img(filename,target_size=(224, 224))\n",
    "        im=np.reshape(im,(224,224,3))\n",
    "        im=im.astype('float32') / 255\n",
    "        Data.append(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T18:32:09.349796Z",
     "start_time": "2020-04-12T18:32:08.172913Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "X_train, X_1, y_train, y_1 = train_test_split(np.array(Data), np.array(Label), test_size=0.3, random_state=42,stratify=Label)\n",
    "\n",
    "#Train Test Split\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_1, y_1, test_size=0.5, random_state=42,stratify=y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T18:32:17.484500Z",
     "start_time": "2020-04-12T18:32:17.474534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print some information about the amount of data\n",
    "len(X_train), len(y_train), len(X_cv), len(y_cv), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the **COVID-19** cases. We load the three subsets directly from their directories because we have splitted them before running the code. The reason of doing this is because we only wanted to use the DataAugmentation technique on the COVID-19 trainig samples, so we had to isolate them from the validation and test COVID-19 images and from the healthy and viral pneumonia ones. \n",
    "\n",
    "After loading each of the three subsets (train, validation and test), we join them with the data from the other two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T18:32:47.376480Z",
     "start_time": "2020-04-12T18:32:25.409358Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For data augmentation COVID with label 0\n",
    "\n",
    "''' TRAINING '''\n",
    "Data=[]\n",
    "Label=[]\n",
    "\n",
    "files = os.listdir(dir_covid_images_train)\n",
    "for file in files:\n",
    "    Label.append(0)\n",
    "    filename=os.path.join(dir_covid_images_train, file)\n",
    "    im=image.load_img(filename,target_size=(224, 224))\n",
    "    im=np.reshape(im,(224,224,3))\n",
    "    im=im.astype('float32') / 255\n",
    "    Data.append(im)\n",
    "\n",
    "y_train = np.array(list(y_train) + Label)\n",
    "X_train = np.array(list(X_train) + Data)\n",
    "\n",
    "''' VALIDATION '''\n",
    "Data=[]\n",
    "Label=[]\n",
    "files = os.listdir(dir_covid_images_val)\n",
    "for file in files:\n",
    "    Label.append(0)\n",
    "    filename=os.path.join(dir_covid_images_val, file)\n",
    "    im=image.load_img(filename,target_size=(224, 224))\n",
    "    im=np.reshape(im,(224,224,3))\n",
    "    im=im.astype('float32') / 255\n",
    "    Data.append(im)\n",
    "\n",
    "y_cv = np.array(list(y_cv) + Label)\n",
    "X_cv = np.array(list(X_cv) + Data)\n",
    "\n",
    "''' TEST '''\n",
    "Data=[]\n",
    "Label=[]\n",
    "files = os.listdir(dir_covid_images_test)\n",
    "for file in files:\n",
    "    Label.append(0)\n",
    "    filename=os.path.join(dir_covid_images_test, file)\n",
    "    im=image.load_img(filename,target_size=(224, 224))\n",
    "    im=np.reshape(im,(224,224,3))\n",
    "    im=im.astype('float32') / 255\n",
    "    Data.append(im)\n",
    "\n",
    "y_test = np.array(list(y_test) + Label)\n",
    "X_test = np.array(list(X_test) + Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T18:32:47.641746Z",
     "start_time": "2020-04-12T18:32:47.636754Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print some information about the amount of data\n",
    "len(X_train), len(y_train), len(X_cv), len(y_cv), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the data has been loaded and divided in train, validation and test sets, we can reshape all the samples to the input shape of the network. A very common shape used in a lot of state in the art publications is **`224 x 224`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T18:32:47.944929Z",
     "start_time": "2020-04-12T18:32:47.897087Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "img_width=224\n",
    "img_height=224\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    \n",
    "    input_shape = (3, img_width, img_height)\n",
    "    X_train = X_train.reshape(X_train.shape[0],3,img_width,img_height)\n",
    "    X_cv    = X_cv.reshape(X_cv.shape[0],3,img_width,img_height)\n",
    "    X_test  = X_test.reshape(X_test.shape[0],3,img_width,img_height)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    input_shape = (img_width, img_height, 3)\n",
    "    X_train = X_train.reshape(X_train.shape[0],img_width,img_height,3)\n",
    "    X_cv    = X_cv.reshape(X_cv.shape[0],img_width,img_height,3)\n",
    "    X_test  = X_test.reshape(X_test.shape[0],img_width,img_height,3)\n",
    "\n",
    "    \n",
    "del Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Architecture\n",
    "\n",
    "In the following cells, the **Baseline Architecture** is defined and trained. Some accuracy values are obtained after the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T16:38:37.087978Z",
     "start_time": "2020-04-11T16:38:36.981242Z"
    }
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(32, 3, input_shape=input_shape, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Conv2D(64, 3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Conv2D(128, 3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Network\n",
    "\n",
    "We are only saving the bests models here, but you can save only the last one or a model per epoch, as you wish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T16:41:23.827120Z",
     "start_time": "2020-04-11T16:38:37.858895Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5', save_best_only=True, monitor='val_acc', mode='max')\n",
    "model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=n_epochs, callbacks=[checkpoint], validation_data=(X_cv,y_cv), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Predictions - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T16:42:13.470271Z",
     "start_time": "2020-04-11T16:42:10.285817Z"
    }
   },
   "outputs": [],
   "source": [
    "train_acc = accuracy_score(model.predict_classes(X_train), y_train)\n",
    "valid_acc = accuracy_score(model.predict_classes(X_cv), y_cv)\n",
    "test_acc  = accuracy_score(model.predict_classes(X_test), y_test)\n",
    "\n",
    "print(\"The final train accuracy is \",train_acc*100,\"%\")\n",
    "print(\"The final validation accuracy is \",valid_acc*100,\"%\")\n",
    "print(\"The final test accuracy is \",test_acc*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T16:42:52.261518Z",
     "start_time": "2020-04-11T16:42:52.201672Z"
    }
   },
   "outputs": [],
   "source": [
    "''' COVID-19 Accuracy '''\n",
    "\n",
    "X_covid  = X_test[y_test == 0]\n",
    "y_covid  = y_test[y_test == 0]\n",
    "test_acc = accuracy_score(model.predict_classes(X_covid), y_covid)\n",
    "\n",
    "print(\"The final test accuracy for COVID-19 is \", test_acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T16:42:52.608576Z",
     "start_time": "2020-04-11T16:42:52.385156Z"
    }
   },
   "outputs": [],
   "source": [
    "''' Healthy Accuracy '''\n",
    "\n",
    "X_normal = X_test[y_test == 1]\n",
    "y_normal = y_test[y_test == 1]\n",
    "test_acc = accuracy_score(model.predict_classes(X_normal), y_normal)\n",
    "\n",
    "print(\"The final test accuracy for NORMAL is \", test_acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T16:42:52.871886Z",
     "start_time": "2020-04-11T16:42:52.662415Z"
    }
   },
   "outputs": [],
   "source": [
    "''' Viral Pneumonia Accuracy '''\n",
    "\n",
    "X_pneumonia = X_test[y_test == 2]\n",
    "y_pneumonia = y_test[y_test == 2]\n",
    "test_acc    = accuracy_score(model.predict_classes(X_pneumonia), y_pneumonia)\n",
    "\n",
    "print(\"The final test accuracy for PNEUMONIA is \", test_acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception Based Architecture\n",
    "\n",
    "**Inception** is a method that applies convolutional layers in parallel with different filter sizes to obtain different features and then concatenates them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T17:52:43.880626Z",
     "start_time": "2020-04-12T17:52:43.765877Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "img_width=224\n",
    "img_height=224\n",
    "input_shape = (img_width, img_height, 3)\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv1 = Conv2D(64, 1, activation='relu', padding='same')(input_layer)\n",
    "conv2 = Conv2D(32, 1, activation='relu', padding='same')(input_layer)\n",
    "conv3 = Conv2D(16, 1, activation='relu', padding='same')(input_layer)\n",
    "\n",
    "layer_out = concatenate([conv1, conv2, conv3], axis=-1)\n",
    "layer_out = MaxPooling2D(2)(layer_out)\n",
    "conv4     = Conv2D(128, 7, activation='relu', padding='same', name='last_conv')(layer_out)\n",
    "layer_out = MaxPooling2D(2)(conv4)\n",
    "layer_out = Flatten()(layer_out)\n",
    "dense1    = Dense(128, activation='relu')(layer_out)\n",
    "dense1    = Dropout(0.3)(dense1)\n",
    "output    = Dense(3, activation='softmax')(dense1)\n",
    "model     = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Network\n",
    "\n",
    "We are only saving the bests models here, but you can save only the last one or a model per epoch, as you wish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T17:50:06.153196Z",
     "start_time": "2020-04-12T17:38:40.219547Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5', save_best_only=True, monitor='val_acc', mode='max')\n",
    "model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=n_epochs, callbacks=[checkpoint], validation_data=(X_cv,y_cv), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Predictions - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T16:42:13.470271Z",
     "start_time": "2020-04-11T16:42:10.285817Z"
    }
   },
   "outputs": [],
   "source": [
    "train_acc = accuracy_score(model.predict_classes(X_train), y_train)\n",
    "valid_acc = accuracy_score(model.predict_classes(X_cv), y_cv)\n",
    "test_acc  = accuracy_score(model.predict_classes(X_test), y_test)\n",
    "\n",
    "print(\"The final train accuracy is \",train_acc*100,\"%\")\n",
    "print(\"The final validation accuracy is \",valid_acc*100,\"%\")\n",
    "print(\"The final test accuracy is \",test_acc*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T16:42:52.261518Z",
     "start_time": "2020-04-11T16:42:52.201672Z"
    }
   },
   "outputs": [],
   "source": [
    "''' COVID-19 Accuracy '''\n",
    "\n",
    "X_covid  = X_test[y_test == 0]\n",
    "y_covid  = y_test[y_test == 0]\n",
    "test_acc = accuracy_score(model.predict_classes(X_covid), y_covid)\n",
    "\n",
    "print(\"The final test accuracy for COVID-19 is \", test_acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T16:42:52.608576Z",
     "start_time": "2020-04-11T16:42:52.385156Z"
    }
   },
   "outputs": [],
   "source": [
    "''' Healthy Accuracy '''\n",
    "\n",
    "X_normal = X_test[y_test == 1]\n",
    "y_normal = y_test[y_test == 1]\n",
    "test_acc = accuracy_score(model.predict_classes(X_normal), y_normal)\n",
    "\n",
    "print(\"The final test accuracy for NORMAL is \", test_acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T16:42:52.871886Z",
     "start_time": "2020-04-11T16:42:52.662415Z"
    }
   },
   "outputs": [],
   "source": [
    "''' Viral Pneumonia Accuracy '''\n",
    "\n",
    "X_pneumonia = X_test[y_test == 2]\n",
    "y_pneumonia = y_test[y_test == 2]\n",
    "test_acc    = accuracy_score(model.predict_classes(X_pneumonia), y_pneumonia)\n",
    "\n",
    "print(\"The final test accuracy for PNEUMONIA is \", test_acc*100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
